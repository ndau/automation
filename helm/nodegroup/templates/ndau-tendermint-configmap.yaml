{{ if .Values.ndau.enabled }}

kind: ConfigMap
metadata:
  name: {{ template "nodegroup.fullname" . }}-ndau-tendermint-config
apiVersion: v1
data:
  genesis.json: {{ b64dec .Values.ndau.genesis | quote }}

  node_key.json: {{ b64dec .Values.ndau.nodeKey | quote }}

  priv_validator_key.json: {{ b64dec .Values.ndau.privValidatorKey | quote }}

  priv_validator_state.json: {{ b64dec .Values.ndau.privValidatorState | quote }}

  make-snapshot.sh: |

    errcho() {
      >&2 echo $@
    }

    already_ran=false

    # s3_upload takes the first argument as the full local file path and uploads it to s3
    # to the path defined by the convention `/{{ .Values.aws.snapshotBucket }}/{{ .Values.networkName }}/$NEW_CODE/$s3_file`, where `$NEW_CODE`
    # is the common random code shared across this release, and $s3_file is the name of
    # the file in s3.
    s3_upload() {

      local local_file=$1
      local s3_file=$2

      # prepare headers and signature
      the_date=$(date -R)
      s3_path="/{{ .Values.aws.snapshotBucket }}/{{ .Values.networkName }}/$NEW_CODE/$s3_file"
      content_type="application/octet-stream"
      signable_bytes="PUT\n\n${content_type}\n${the_date}\n${s3_path}"
      signature=$(echo -en $signable_bytes | openssl sha1 -hmac $AWS_SECRET -binary | base64)

      # upload the manifest
      curl -X PUT -T "$local_file" \
        -H "Host: s3.amazonaws.com" \
        -H "Date: $the_date" \
        -H "Content-Type: $content_type" \
        -H "Authorization: AWS $AWS_KEY:$signature" \
        http://s3.amazonaws.com$s3_path

    }

    this_node=ndau
    this_app=tm
    this_service=$this_node-$this_app

    # Keep this as chaos so everyone is using the same database
    rhost={{ template "nodegroup.fullname" . }}-chaos-redis-service

    # wait for redis
    errcho "Pinging redis"
    while true ; do
      # get redis readiness
      if ! redis-cli -h $rhost PING; then
        errcho "Waiting for redis to pong."
        sleep 1
      else
        errcho "Redis is ready."
        break
      fi
    done

    # curl retry options
    CURL_CONNECT_TIMEOUT=5  # each try waits X seconds
    CURL_RETRY_MAX=50       # retry X many times
    CURL_RETRY_TOTAL=1000   # arbitrary high number, it will timeout first.
    CURL_RETRY_DELAY=5      # try every X seconds
    CURL_TOTAL_TIMEOUT=60   # total seconds before it fails (60s=1m) This overrides everything.

    # snapshot runs when SIGTERM is sent to this process
    snapshot() {

      already_ran=true

      # get the manifest variables
      status_response=$(curl --connect-timeout $CURL_CONNECT_TIMEOUT \
            --retry-connrefused \
            --max-time $CURL_RETRY_MAX \
            --retry $CURL_RETRY_TOTAL \
            --retry-delay $CURL_RETRY_DELAY \
            --retry-max-time $CURL_TOTAL_TIMEOUT \
            localhost:26657/status)
      errcho "Tendermint status response: $status_response"
      height=$(jq -r ".result.sync_info.latest_block_height" <<< $status_response)
      errcho "height: $height"
      app_hash=$(jq -r ".result.sync_info.latest_app_hash" <<< $status_response)
      errcho "app_hash: $app_hash"

      # try to set snapping flag if not set. If set, get it.
      # Everything expires after 120 seconds. If things get hung up, it's possible to restart.
      snap_res=$(redis-cli -h $rhost SET "snapshot-$NEW_CODE-snapping" 1 EX 120 NX)
      errcho "snap_res: $snap_res"
      if [ "$snap_res" == "OK" ]; then
        # check to see if this height was already uploaded
        if curl -f http://s3.amazonaws.com/{{ .Values.aws.snapshotBucket }}/{{ .Values.networkName }}/by-height/${this_node}-${height}; then
          errcho "Quitting. Snapshot for this height ($height) already uploaded."
          exit 1
        fi

        errcho "Beginning new snapshot process for all services."
        snapping=1
        redis-cli -h $rhost SET snapshot-$NEW_CODE-ndau-tm 1 EX 120
        redis-cli -h $rhost SET snapshot-$NEW_CODE-ndau-noms 1 EX 120
        redis-cli -h $rhost SET snapshot-$NEW_CODE-chaos-tm 1 EX 120
        redis-cli -h $rhost SET snapshot-$NEW_CODE-chaos-noms 1 EX 120

      else
        # could return either 1 or 0 on second run
        snapping=$(redis-cli -h $rhost GET "snapshot-$NEW_CODE-snapping")
        errcho "Snapping already in progress: snapshot-$NEW_CODE-snapping = $snapping"
      fi

      # wait until noms snapshot is done
      noms_snapshot_url=http://s3.amazonaws.com/{{ .Values.aws.snapshotBucket }}/{{ .Values.networkName }}/$NEW_CODE/${this_node}-noms.tgz
      errcho "Waiting for noms snapshot to appear at: $noms_snapshot_url."

      # curl until the noms snapshot is up, or CURL_TOTAL_TIMEOUT passes
      if curl --head --connect-timeout $CURL_CONNECT_TIMEOUT \
          --retry-connrefused \
          --max-time $CURL_RETRY_MAX \
          --retry $CURL_RETRY_TOTAL \
          --retry-delay $CURL_RETRY_DELAY \
          --retry-max-time $CURL_TOTAL_TIMEOUT \
          $noms_snapshot_url; then
          errcho "Done waiting. Detected noms snapshot."
      else
          errcho "Aborting. Noms snapshot not detected."
          exit 1
      fi


      # upload the tendermint databases

      # check to see if the snapshot is already there
      tar_file=$this_service.tgz
      s3_path="/{{ .Values.aws.snapshotBucket }}/{{ .Values.networkName }}/$NEW_CODE/${tar_file}"

      if curl_response=$(curl -f http://s3.amazonaws.com$s3_path); then
        errcho "Snapshot $s3_path already exists. Will not upload."
      else
        errcho "Uploading snapshot $s3_path."

        (
          cd /tendermint/data
          tar cvzf /root/$tar_file ./blockstore.db ./state.db
        )

        # prepare headers and signature
        the_date=$(date -R)
        content_type="application/octet-stream"
        signable_bytes="PUT\n\n${content_type}\n${the_date}\n${s3_path}"
        signature=$(echo -en $signable_bytes | openssl sha1 -hmac $AWS_SECRET -binary | base64)

        # upload tarball to s3
        errcho "Uploading snapshot $s3_path."
        curl -X PUT -T "/root/$tar_file" \
          -H "Host: s3.amazonaws.com" \
          -H "Date: $the_date" \
          -H "Content-Type: $content_type" \
          -H "Authorization: AWS $AWS_KEY:$signature" \
          http://s3.amazonaws.com$s3_path

      fi


      # upload genesis

      tar_file=${this_node}-genesis.tgz

      # make the tar ball
      (
        cd /tendermint/config
        tar cvzf /root/$tar_file ./genesis.json
      )

      # prepare headers and signature
      the_date=$(date -R)
      s3_path="/{{ .Values.aws.snapshotBucket }}/{{ .Values.networkName }}/$NEW_CODE/${tar_file}"
      content_type="application/octet-stream"
      signable_bytes="PUT\n\n${content_type}\n${the_date}\n${s3_path}"
      signature=$(echo -en $signable_bytes | openssl sha1 -hmac $AWS_SECRET -binary | base64)

      # upload tarball to s3
      errcho "Uploading genesis to $s3_path."
      curl -X PUT -T "/root/$tar_file" \
        -H "Host: s3.amazonaws.com" \
        -H "Date: $the_date" \
        -H "Content-Type: $content_type" \
        -H "Authorization: AWS $AWS_KEY:$signature" \
        http://s3.amazonaws.com$s3_path

      # remove service flag
      redis-cli -h $rhost DEL "snapshot-$NEW_CODE-$this_service"

      # if everyone is done, then remove snapping key
      if ! redis-cli -h $rhost MGET \
        snapshot-$NEW_CODE-ndau-tm \
        snapshot-$NEW_CODE-ndau-noms \
        snapshot-$NEW_CODE-chaos-tm \
        snapshot-$NEW_CODE-chaos-noms | grep 1; then

        errcho "No current snapshot jobs."

        # we're all done
        errcho "Removing snapping flag."
        redis-cli -h $rhost DEL snapshot-$NEW_CODE-snapping

      else
        errcho "Other snapshot jobs still in progress."
      fi

      # create a manifest file
      jq --arg date "$(date)" \
        --arg height "$height" \
        --arg app_hash "$app_hash" \
        ". | \
        .[\"date\"]=\$date | \
        .[\"height\"]=\$height | \
        .[\"app_hash\"]=\$app_hash" \
        <<< '{}' > /root/manifest.json
      errcho "Created manifest file: $(cat /root/manifest.json)"

      # prepare headers and signature
      the_date=$(date -R)
      s3_path="/{{ .Values.aws.snapshotBucket }}/{{ .Values.networkName }}/$NEW_CODE/${this_node}-manifest.json"
      content_type="application/octet-stream"
      signable_bytes="PUT\n\n${content_type}\n${the_date}\n${s3_path}"
      signature=$(echo -en $signable_bytes | openssl sha1 -hmac $AWS_SECRET -binary | base64)

      # upload to s3
      curl -X PUT -T "/root/manifest.json" \
        -H "Host: s3.amazonaws.com" \
        -H "Date: $the_date" \
        -H "Content-Type: $content_type" \
        -H "Authorization: AWS $AWS_KEY:$signature" \
        http://s3.amazonaws.com$s3_path

      # upload block height key

      # prepare headers and signature
      the_date=$(date -R)
      s3_path="/{{ .Values.aws.snapshotBucket }}/{{ .Values.networkName }}/by-height/${this_node}-${height}"
      content_type="application/octet-stream"
      signable_bytes="PUT\n\n${content_type}\n${the_date}\n${s3_path}"
      signature=$(echo -en $signable_bytes | openssl sha1 -hmac $AWS_SECRET -binary | base64)

      # upload to s3
      curl -X PUT -T "/root/new_code" \
        -H "Host: s3.amazonaws.com" \
        -H "Date: $the_date" \
        -H "Content-Type: $content_type" \
        -H "Authorization: AWS $AWS_KEY:$signature" \
        http://s3.amazonaws.com$s3_path

    }

    errcho "Waiting to snapshot"

    # trap kubernetes shutdown signal. After grace period it will send sigkill.
    trap snapshot SIGTERM

    # check redis and loop until this service has been told to snapshot
    while [ "$res" != "1" ] && ! $already_ran; do
      if $already_ran; then
        errcho "Caught SIGTERM, not waiting."
        exit 1
      fi
      printf "."
      sleep 10
      res=$(timeout -t 5 redis-cli -h $rhost get "snapshot-$NEW_CODE-$this_service")
    done

    errcho "Done waiting. This value for key snapshot-$NEW_CODE-$this_service: $res"

    # don't try to snapshot if we snapshotted already with sigterm
    $already_ran || snapshot


  config.toml: |
    # This is a TOML config file.
    # For more information, see https://github.com/toml-lang/toml

    ##### main base config options #####

    # TCP or UNIX socket address of the ABCI application,
    # or the name of an ABCI application compiled in with the Tendermint binary
    proxy_app = "tcp://{{ include "nodegroup.fullname" . }}-ndaunode-service:{{ .Values.ndaunode.port }}"

    # A custom human readable name for this node
    moniker = {{ .Values.ndau.tendermint.moniker | quote }}

    # If this node is many blocks behind the tip of the chain, FastSync
    # allows them to catchup quickly by downloading blocks in parallel
    # and verifying their commits
    fast_sync = true

    # Database backend: leveldb | memdb | cleveldb
    db_backend = "leveldb"

    # Database directory
    db_dir = "data"

    # Output level for logging, including package level options
    log_level = "main:info,state:info,*:error"

    # Output format: 'plain' (colored text) or 'json'
    log_format = "plain"

    ##### additional base config options #####

    # Path to the JSON file containing the initial validator set and other meta data
    genesis_file = "config/genesis.json"

    # Path to the JSON file containing the private key to use as a validator in the consensus protocol
    priv_validator_key_file = "config/priv_validator_key.json"

    # Path to the JSON file containing the last sign state of a validator
    priv_validator_state_file = "data/priv_validator_state.json"

    # TCP or UNIX socket address for Tendermint to listen on for
    # connections from an external PrivValidator process
    priv_validator_laddr = ""

    # Path to the JSON file containing the private key to use for node authentication in the p2p protocol
    node_key_file = "config/node_key.json"

    # Mechanism to connect to the ABCI application: socket | grpc
    abci = "socket"

    # TCP or UNIX socket address for the profiling server to listen on
    prof_laddr = ""

    # If true, query the ABCI app on connecting to a new peer
    # so the app can decide if we should keep the connection or not
    filter_peers = false

    ##### advanced configuration options #####

    ##### rpc server configuration options #####
    [rpc]

    # TCP or UNIX socket address for the RPC server to listen on
    laddr = "tcp://0.0.0.0:{{ required ".Values.ndau.tendermint.ports.rpc required" .Values.ndau.tendermint.ports.rpc }}"

    # A list of origins a cross-domain request can be executed from
    # Default value '[]' disables cors support
    # Use '["*"]' to allow any origin
    cors_allowed_origins = []

    # A list of methods the client is allowed to use with cross-domain requests
    cors_allowed_methods = ["HEAD", "GET", "POST", ]

    # A list of non simple headers the client is allowed to use with cross-domain requests
    cors_allowed_headers = ["Origin", "Accept", "Content-Type", "X-Requested-With", "X-Server-Time", ]

    # TCP or UNIX socket address for the gRPC server to listen on
    # NOTE: This server only supports /broadcast_tx_commit
    grpc_laddr = ""

    # Maximum number of simultaneous connections.
    # Does not include RPC (HTTP&WebSocket) connections. See max_open_connections
    # If you want to accept a larger number than the default, make sure
    # you increase your OS limits.
    # 0 - unlimited.
    # Should be < {ulimit -Sn} - {MaxNumInboundPeers} - {MaxNumOutboundPeers} - {N of wal, db and other open files}
    # 1024 - 40 - 10 - 50 = 924 = ~900
    grpc_max_open_connections = 900

    # Activate unsafe RPC commands like /dial_seeds and /unsafe_flush_mempool
    unsafe = false

    # Maximum number of simultaneous connections (including WebSocket).
    # Does not include gRPC connections. See grpc_max_open_connections
    # If you want to accept a larger number than the default, make sure
    # you increase your OS limits.
    # 0 - unlimited.
    # Should be < {ulimit -Sn} - {MaxNumInboundPeers} - {MaxNumOutboundPeers} - {N of wal, db and other open files}
    # 1024 - 40 - 10 - 50 = 924 = ~900
    max_open_connections = 900

    ##### peer to peer configuration options #####
    [p2p]

    # Address to listen for incoming connections
    laddr = "tcp://0.0.0.0:{{ required ".Values.ndau.tendermint.ports.p2p required" .Values.ndau.tendermint.ports.p2p }}"

    # Address to advertise to peers for them to dial
    # If empty, will use the same port as the laddr,
    # and will introspect on the listener or use UPnP
    # to figure out the address.
    external_address = ""

    # Comma separated list of seed nodes to connect to
    seeds = ""

    # Comma separated list of nodes to keep persistent connections to
    persistent_peers = ""

    # UPNP port forwarding
    upnp = false

    # Path to address book
    addr_book_file = "config/addrbook.json"

    # Set true for strict address routability rules
    # Set false for private or local networks
    addr_book_strict = false # fet: false means multiple nodes can run on the same ip

    # Maximum number of inbound peers
    max_num_inbound_peers = 40

    # Maximum number of outbound peers to connect to, excluding persistent peers
    max_num_outbound_peers = 10

    # Time to wait before flushing messages out on the connection
    flush_throttle_timeout = "100ms"

    # Maximum size of a message packet payload, in bytes
    max_packet_msg_payload_size = 1024

    # Rate at which packets can be sent, in bytes/second
    send_rate = 5120000

    # Rate at which packets can be received, in bytes/second
    recv_rate = 5120000

    # Set true to enable the peer-exchange reactor
    pex = true

    # Seed mode, in which node constantly crawls the network and looks for
    # peers. If another node asks it for addresses, it responds and disconnects.
    #
    # Does not work if the peer-exchange reactor is disabled.
    seed_mode = false

    # Comma separated list of peer IDs to keep private (will not be gossiped to other peers)
    private_peer_ids = ""

    # Toggle to disable guard against peers connecting from the same ip.
    allow_duplicate_ip = true

    # Peer connection configuration.
    handshake_timeout = "20s"
    dial_timeout = "3s"

    ##### mempool configuration options #####
    [mempool]

    recheck = true
    broadcast = true
    wal_dir = ""

    # size of the mempool
    size = 5000

    # size of the cache (used to filter transactions we saw earlier)
    cache_size = 10000

    ##### consensus configuration options #####
    [consensus]

    wal_file = "data/cs.wal/wal"

    timeout_propose = "3s"
    timeout_propose_delta = "500ms"
    timeout_prevote = "1s"
    timeout_prevote_delta = "500ms"
    timeout_precommit = "1s"
    timeout_precommit_delta = "500ms"
    timeout_commit = "1s"

    # Make progress as soon as we have all the precommits (as if TimeoutCommit = 0)
    skip_timeout_commit = false

    # EmptyBlocks mode and possible interval between empty blocks
    # create_empty_blocks = true
    create_empty_blocks_interval = "{{ .Values.ndau.tendermint.createEmptyBlocksInterval }}"

    # Reactor sleep duration parameters
    peer_gossip_sleep_duration = "100ms"
    peer_query_maj23_sleep_duration = "2s"

    # Block time parameters. Corresponds to the minimum time increment between consecutive blocks.
    blocktime_iota = "1s"

    ##### transactions indexer configuration options #####
    [tx_index]

    # What indexer to use for transactions
    #
    # Options:
    #   1) "null"
    #   2) "kv" (default) - the simplest possible indexer, backed by key-value storage (defaults to levelDB; see DBBackend).
    indexer = "kv"

    # Comma-separated list of tags to index (by default the only tag is "tx.hash")
    #
    # You can also index transactions by height by adding "tx.height" tag here.
    #
    # It's recommended to index only a subset of tags due to possible memory
    # bloat. This is, of course, depends on the indexer's DB and the volume of
    # transactions.
    index_tags = ""

    # When set to true, tells indexer to index all tags (predefined tags:
    # "tx.hash", "tx.height" and all tags from DeliverTx responses).
    #
    # Note this may be not desirable (see the comment above). IndexTags has a
    # precedence over IndexAllTags (i.e. when given both, IndexTags will be
    # indexed).
    index_all_tags = false

    ##### instrumentation configuration options #####
    [instrumentation]

    # When true, Prometheus metrics are served under /metrics on
    # PrometheusListenAddr.
    # Check out the documentation for the list of available metrics.
    prometheus = false

    # Address to listen for Prometheus collector(s) connections
    prometheus_listen_addr = ":26660"

    # Maximum number of simultaneous connections.
    # If you want to accept a larger number than the default, make sure
    # you increase your OS limits.
    # 0 - unlimited.
    max_open_connections = 3

    # Instrumentation namespace
    namespace = "tendermint"

  tendermint-start.sh: |
    #!/bin/sh

    # Tendermint config
    /bin/sh /root/tendermint-init.sh

    # wait for ndaunode to accept requests
    while ! nc -z {{ template "nodegroup.fullname" . }}-ndaunode-service {{ .Values.ndaunode.port }}; do
      sleep 1
    done

    # start tendermint, add --log_level="*:debug" to debug
    /bin/tendermint node \
      --moniker={{ .Values.ndau.tendermint.moniker }} \
      --p2p.private_peer_ids "$PRIVATE_PEER_IDS" \
      --p2p.persistent_peers "$PERSISTENT_PEERS" \
      --log_level="*:debug" &

    tendermint_pid=$!

    delayed_shutdown() {
      sleep 60 # sleep for 1 minute
      kill $tendermint_pid
    }

    # do a delayed shutdown when the child process exits as well
    trap delayed_shutdown CHLD

    # loop forever for two reasons
    # 1) main process stays open
    # 2) trap will wait for a sigterm and run delayed_shutdown
    trap delayed_shutdown SIGTERM
    while true; do sleep 42; done

{{ end }}
